{
    "contents" : "## Program     : Coursera Capston Project Task 1\n## Written By  : Gabriel Mohanna\n## Date Created: Oct 28, 2014\n##\n## Narrative   : Cleaning, tokenize and profanity filtering\n##\n## \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< Code is Poetry >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n## //////////////////////////////////////////////////////////////////////////////////////////////////////////////////////\n##\n## **********************************************************************************************************************\n## Steps\n## -----\n## (0) Define Working Space\n## (1) Examine Data\n## (2) Transform Text\n## (3) Tokenize and Build Term-Document Matrix\n##\n# ******************************************************************************************************\n\n# ******************************************************************************************************\n# (0) Define Working Space\n# ******************************************************************************************************\n# Clear Workspace\nrm(list=ls())\n\n## Define URL Location\n#url.location <- \"http://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip\"\n#zip.name     <- \"Coursera-SwiftKey.zip\"\n\n# Define Paths\nroot      <- \"/users/gabrielm/\"\nsub.root  <- \"OneDrive/Documents/HW/Coursera/Data Science Specialization/10 - Capstone Project/\"\ndata.path <- \"Data/\"\n\n# Define training data percentage\ninTrain.p <- .6\n\n# Load Libraries\nlibrary(tm)\nlibrary(SnowballC)\nlibrary(RWeka)\nlibrary(wordcloud)\nlibrary(ggplot2)\nlibrary(corrplot)\nlibrary(magrittr)\n#library(R.utils)\n\n# End Define Working Space\n\n\n# ******************************************************************************************************\n# (1) Examine Data\n# ******************************************************************************************************\nsetwd(paste0(root, sub.root, data.path))\n\n# Read Data\nus.twitter <- readLines(\"./en_US/en_US.twitter.txt\", n = 5000, warn=F)\n\n## Select Data\n#set.seed(123)\n#inTrain <- rep(F, length(us.twitter))\n#inTrain <- unlist(lapply(inTrain, \n#                         function(x) {\n#                             ifelse(runif(1) < inTrain.p, T, F)\n#                         }))\n#us.twitter[inTrain]\n\n# Print Lines\nprintLines <- function(file, line, width=78) {\n    if(missing(line)) \n        line <- 1:length(file)\n    \n    for(i in line) {\n        cat(paste(\"[[\", i, \"]] \", sep=\"\"))\n        writeLines(strwrap(file[[i]], width=width))\n    }\n}\n\nprintLines(us.twitter, 1)\n\n# End Examine Data\n\n\n# ******************************************************************************************************\n# (2) Transform Text\n# ******************************************************************************************************\n# Build a Corpus, Specify the Source to be Character Vectors\nus.twitter.corpus <- Corpus(VectorSource(us.twitter))\n\n# Clean Corpus\nus.twitter.corpus <- tm_map(us.twitter.corpus, stripWhitespace)\nus.twitter.corpus <- tm_map(us.twitter.corpus, removeNumbers)\nus.twitter.corpus <- tm_map(us.twitter.corpus, removePunctuation)\nus.twitter.corpus <- tm_map(us.twitter.corpus, content_transformer(tolower))\nus.twitter.corpus <- tm_map(us.twitter.corpus, content_transformer(function(x) gsub(\"http[[:alnum:]]*\", \"\", x)))\n\nprofanity <- read.table(\"bad-words.txt\", stringsAsFactors = F)\nus.twitter.corpus <- tm_map(us.twitter.corpus, removeWords, c(stopwords(\"english\"), profanity[[1]]))\n\n# End Transform Text\n\n\n# ******************************************************************************************************\n# (3) Tokenize and Build Term-Document Matrix\n# ******************************************************************************************************\n# Stem/Tokenize\nus.twitter.corpus <- tm_map(us.twitter.corpus, stemDocument)\n#inspect(us.twitter.corpus[1])\n\n# Document-Term Matrix\nus.twitter.dtm <- DocumentTermMatrix(us.twitter.corpus)\n\n# TF-IDF Matrix\nus.twitter.tfidf <- weightTfIdf(us.twitter.dtm)\n\n# End Tokenize and Build Term-Document Matrix\n\n\n# ******************************************************************************************************\n# (4) Terms Frequencies\n# ******************************************************************************************************\nfreq <- colSums(as.matrix(us.twitter.dtm))\nord <- order(freq)\nfreq[tail(ord)]\nfindFreqTerms(us.twitter.dtm, lowfreq = 100)\n\ntermFrequency <- colSums(as.matrix(us.twitter.dtm))\ntermFrequency <- subset(termFrequency, termFrequency>=80)\ntf.df <- data.frame(term=names(termFrequency), freq=termFrequency)\n\nsubset(tf.df, freq>=80) %>% \n    ggplot(aes(term, freq)) + geom_bar(stat=\"identity\") + coord_flip()\n\nsubset(tf.df, freq>=50) %>% \n    ggplot(aes(term, freq)) + geom_bar(stat=\"identity\") + coord_flip()\n\n# End Terms Frequencies\n\n\n# ******************************************************************************************************\n# (5) WordCloud\n# ******************************************************************************************************\nwordFreq <- us.twitter.dtm %>% as.matrix %>% colSums %>% sort(decreasing = T)\n\nset.seed(123)\nwordcloud(words=names(wordFreq), freq=wordFreq, random.order=F, \n          min.freq = 3, max.words = 100, \n          colors = brewer.pal(6, \"Dark2\"),\n          rot.per=.2)\n\n# End WordCloud\n\n\n# ******************************************************************************************************\n# (6) Words Association & Correlations\n# ******************************************************************************************************\n# Association\nfindAssocs(us.twitter.dtm, \"last\", .2)\nfindAssocs(us.twitter.dtm, \"love\", .2)\nfindAssocs(us.twitter.dtm, \"hate\", .2)\nfindAssocs(us.twitter.dtm, \"work\", .1)\n\n# Remove Sparse Terms\nus.twitter.dtm.NonSparse <- us.twitter.dtm %>% removeSparseTerms(sparse = .98)\nus.twitter.dtm.NonSparse$dimnames$Terms\n\n# Correlations\ncorrplot(corr = cor(as.matrix(us.twitter.dtm.NonSparse)))\n?corrplot\ncorrplot.mixed(corr = cor(as.matrix(us.twitter.dtm.NonSparse)), upper = \"shade\", tl.cex=.5)\n# End Words Association & Correlations\n\n\n# ******************************************************************************************************\n# (7) Clustering Words\n# ******************************************************************************************************\n# Remove Sparse Terms\nus.twitter.dtm.NonSparse <- us.twitter.dtm %>% removeSparseTerms(sparse = .98)\nus.twitter.dtm.NonSparse$dimnames$Terms\n\n# Calculate Distances\nus.twitter.dist <- us.twitter.dtm.NonSparse %>% as.matrix %>% t %>% scale %>% dist\n\n# Plot Clusters\nus.twitter.dist %>% hclust(method=\"ward.D\" ) %>% plot(main=\"Hier. Clust: ward.D\")\nus.twitter.dist %>% hclust(method=\"ward.D2\") %>% plot(main=\"Hier. Clust: ward.D2\")\nus.twitter.dist %>% hclust                   %>% plot(main=\"Hier. Clust: default\")\n\n# End Clustering Words\n\n\n# ******************************************************************************************************\n# (8) n-Grams\n# ******************************************************************************************************\n# N-grams Tokenization\nBigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\nus.twitter.ngram2 <- DocumentTermMatrix(us.twitter.corpus, control = list(tokenize = BigramTokenizer))\ninspect(us.twitter.ngram2)\n\n# Remove Sparse Terms\nus.twitter.ngram2.NonSparse <- us.twitter.ngram2 %>% removeSparseTerms(sparse = .999)\n\ninspect(us.twitter.ngram2.NonSparse[1:20, 1:8])\n",
    "created" : 1414778857779.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4214343932",
    "id" : "7BA0ADFC",
    "lastKnownWriteTime" : 1414780804,
    "path" : "~/OneDrive/Documents/HW/Coursera/Data Science Specialization/10 - Capstone Project/Code/Task 1.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}